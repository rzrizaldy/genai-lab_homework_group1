{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Part B: Zero-Shot, Few-Shot & Chain-of-Thought Prompting\n\n**Experiments can be conducted using either:**\n1. **Automated:** HuggingFace Inference API (uncomment code to run automatically)\n2. **Manual:** https://gpt-oss.com/ (copy prompts, paste predictions)\n\nFor automated method, set `HF_TOKEN` in `.env` file and uncomment `get_completion()` calls."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nimport pandas as pd\n\nload_dotenv()\n\n# Initialize client with HuggingFace router (for automated inference)\nclient = OpenAI(\n    base_url=\"https://router.huggingface.co/v1\",\n    api_key=os.environ.get(\"HF_TOKEN\", \"your_token_here\"),\n)\n\ndef get_completion(prompt, model=\"openai/gpt-oss-20b:groq\"):\n    \"\"\"Get completion from HuggingFace inference API\"\"\"\n    try:\n        completion = client.chat.completions.create(\n            model=model,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n        )\n        return completion.choices[0].message.content\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n# Training examples - use these in your few-shot prompts\ntrain_examples = [\n    {\"text\": \"(Lawrence bounces) All over the stage, dancing, running, sweating, mopping his face and generally displaying the wacky talent that brought him fame in the first place.\", \"label\": \"positive\"},\n    {\"text\": \"Despite all evidence to the contrary, this clunker has somehow managed to pose as an actual feature movie, the kind that charges full admission and gets hyped on tv and purports to amuse small children and ostensible adults.\", \"label\": \"negative\"},\n    {\"text\": \"For the first time in years, de niro digs deep emotionally, perhaps because he's been stirred by the powerful work of his co-stars.\", \"label\": \"positive\"},\n    {\"text\": \"I'll bet the video game is a lot more fun than the film.\", \"label\": \"negative\"},\n    {\"text\": \"A masterpiece of storytelling with brilliant performances throughout.\", \"label\": \"positive\"}\n]\n\n# Test examples - evaluate your prompts on these\ntest_examples = [\n    {\"text\": \"An absolute waste of time and money. Terrible acting and nonsensical plot.\", \"label\": \"negative\"},\n    {\"text\": \"Captivating from start to finish. A must-see film!\", \"label\": \"positive\"},\n    {\"text\": \"Boring and predictable. I fell asleep halfway through.\", \"label\": \"negative\"},\n    {\"text\": \"Brilliant cinematography and a touching story that stays with you.\", \"label\": \"positive\"},\n    {\"text\": \"The worst movie I've seen this year. Complete disaster.\", \"label\": \"negative\"}\n]\n\nground_truth = [ex['label'] for ex in test_examples]\n\nprint(f\"Training set: {len(train_examples)} examples\")\nprint(f\"Test set: {len(test_examples)} examples\")\nprint(f\"\\nTest examples:\")\nfor i, ex in enumerate(test_examples, 1):\n    print(f\"{i}. [{ex['label']}] {ex['text'][:50]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 1: Zero-Shot (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "prompt_template = \"\"\"Classify the sentiment of the following movie review as either 'positive' or 'negative'.\n",
    "\n",
    "Review: {text}\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "# Print prompt for Test 1\n",
    "print(prompt_template.format(text=test_examples[0]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Option 1: Run automated (uncomment to use)\n# zero_shot_predictions = []\n# for ex in test_examples:\n#     prompt = f\"\"\"Classify the sentiment of the following movie review as either 'positive' or 'negative'.\n# \n# Review: {ex['text']}\n# Sentiment:\"\"\"\n#     response = get_completion(prompt).strip().lower()\n#     zero_shot_predictions.append(response)\n\n# Option 2: Paste predictions manually\nzero_shot_predictions = [\n    \"negative\",  # Test 1\n    \"positive\",  # Test 2\n    \"negative\",  # Test 3\n    \"positive\",  # Test 4\n    \"negative\"   # Test 5\n]\n\n# Calculate accuracy\ncorrect = sum([1 for pred, truth in zip(zero_shot_predictions, ground_truth) if pred == truth])\naccuracy = correct / len(ground_truth)\n\nprint(f\"Zero-Shot Results:\")\nfor i, (pred, truth) in enumerate(zip(zero_shot_predictions, ground_truth), 1):\n    status = \"✓\" if pred == truth else \"✗\"\n    print(f\"Test {i}: {status} Predicted={pred}, Expected={truth}\")\nprint(f\"\\nAccuracy: {correct}/{len(ground_truth)} = {accuracy:.1%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 2: One-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "prompt_template = \"\"\"Classify the sentiment of movie reviews as either 'positive' or 'negative'.\n",
    "\n",
    "Review: {train_ex1}\n",
    "Sentiment: {train_label1}\n",
    "\n",
    "Review: {test_text}\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "# Print prompt for Test 1\n",
    "print(prompt_template.format(\n",
    "    train_ex1=train_examples[0]['text'],\n",
    "    train_label1=train_examples[0]['label'],\n",
    "    test_text=test_examples[0]['text']\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your predictions here\n",
    "one_shot_predictions = [\n",
    "    \"\",  # Test 1\n",
    "    \"\",  # Test 2\n",
    "    \"\",  # Test 3\n",
    "    \"\",  # Test 4\n",
    "    \"\"   # Test 5\n",
    "]\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = sum([1 for pred, truth in zip(one_shot_predictions, ground_truth) if pred == truth])\n",
    "accuracy = correct / len(ground_truth)\n",
    "\n",
    "print(f\"One-Shot Results:\")\n",
    "for i, (pred, truth) in enumerate(zip(one_shot_predictions, ground_truth), 1):\n",
    "    status = \"✓\" if pred == truth else \"✗\"\n",
    "    print(f\"Test {i}: {status} Predicted={pred}, Expected={truth}\")\n",
    "print(f\"\\nAccuracy: {correct}/{len(ground_truth)} = {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 3: Three-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template with 3 examples\n",
    "prompt_template = \"\"\"Classify the sentiment of movie reviews as either 'positive' or 'negative'.\n",
    "\n",
    "Review: {train_ex1}\n",
    "Sentiment: {train_label1}\n",
    "\n",
    "Review: {train_ex2}\n",
    "Sentiment: {train_label2}\n",
    "\n",
    "Review: {train_ex3}\n",
    "Sentiment: {train_label3}\n",
    "\n",
    "Review: {test_text}\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "# Print prompt for Test 1\n",
    "print(prompt_template.format(\n",
    "    train_ex1=train_examples[0]['text'],\n",
    "    train_label1=train_examples[0]['label'],\n",
    "    train_ex2=train_examples[1]['text'],\n",
    "    train_label2=train_examples[1]['label'],\n",
    "    train_ex3=train_examples[2]['text'],\n",
    "    train_label3=train_examples[2]['label'],\n",
    "    test_text=test_examples[0]['text']\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your predictions here\n",
    "three_shot_predictions = [\n",
    "    \"\",  # Test 1\n",
    "    \"\",  # Test 2\n",
    "    \"\",  # Test 3\n",
    "    \"\",  # Test 4\n",
    "    \"\"   # Test 5\n",
    "]\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = sum([1 for pred, truth in zip(three_shot_predictions, ground_truth) if pred == truth])\n",
    "accuracy = correct / len(ground_truth)\n",
    "\n",
    "print(f\"Three-Shot Results:\")\n",
    "for i, (pred, truth) in enumerate(zip(three_shot_predictions, ground_truth), 1):\n",
    "    status = \"✓\" if pred == truth else \"✗\"\n",
    "    print(f\"Test {i}: {status} Predicted={pred}, Expected={truth}\")\n",
    "print(f\"\\nAccuracy: {correct}/{len(ground_truth)} = {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 4: Five-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template with all 5 training examples\n",
    "prompt_parts = [\"Classify the sentiment of movie reviews as either 'positive' or 'negative'.\\n\"]\n",
    "for ex in train_examples:\n",
    "    prompt_parts.append(f\"\\nReview: {ex['text']}\\nSentiment: {ex['label']}\")\n",
    "\n",
    "prompt_parts.append(f\"\\n\\nReview: {test_examples[0]['text']}\\nSentiment:\")\n",
    "print(\"\".join(prompt_parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your predictions here\n",
    "five_shot_predictions = [\n",
    "    \"\",  # Test 1\n",
    "    \"\",  # Test 2\n",
    "    \"\",  # Test 3\n",
    "    \"\",  # Test 4\n",
    "    \"\"   # Test 5\n",
    "]\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = sum([1 for pred, truth in zip(five_shot_predictions, ground_truth) if pred == truth])\n",
    "accuracy = correct / len(ground_truth)\n",
    "\n",
    "print(f\"Five-Shot Results:\")\n",
    "for i, (pred, truth) in enumerate(zip(five_shot_predictions, ground_truth), 1):\n",
    "    status = \"✓\" if pred == truth else \"✗\"\n",
    "    print(f\"Test {i}: {status} Predicted={pred}, Expected={truth}\")\n",
    "print(f\"\\nAccuracy: {correct}/{len(ground_truth)} = {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 5: Shuffled Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffled order: 4, 5, 1, 3, 2\n",
    "shuffled_order = [3, 4, 0, 2, 1]\n",
    "shuffled_examples = [train_examples[i] for i in shuffled_order]\n",
    "\n",
    "prompt_parts = [\"Classify the sentiment of movie reviews as either 'positive' or 'negative'.\\n\"]\n",
    "for ex in shuffled_examples:\n",
    "    prompt_parts.append(f\"\\nReview: {ex['text']}\\nSentiment: {ex['label']}\")\n",
    "\n",
    "prompt_parts.append(f\"\\n\\nReview: {test_examples[0]['text']}\\nSentiment:\")\n",
    "print(\"\".join(prompt_parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your predictions here\n",
    "shuffled_predictions = [\n",
    "    \"\",  # Test 1\n",
    "    \"\",  # Test 2\n",
    "    \"\",  # Test 3\n",
    "    \"\",  # Test 4\n",
    "    \"\"   # Test 5\n",
    "]\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = sum([1 for pred, truth in zip(shuffled_predictions, ground_truth) if pred == truth])\n",
    "accuracy = correct / len(ground_truth)\n",
    "\n",
    "print(f\"Shuffled Examples Results:\")\n",
    "for i, (pred, truth) in enumerate(zip(shuffled_predictions, ground_truth), 1):\n",
    "    status = \"✓\" if pred == truth else \"✗\"\n",
    "    print(f\"Test {i}: {status} Predicted={pred}, Expected={truth}\")\n",
    "print(f\"\\nAccuracy: {correct}/{len(ground_truth)} = {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 6: Mislabeled Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip labels for first 3 training examples\n",
    "mislabeled_examples = [\n",
    "    {\"text\": train_examples[0]['text'], \"label\": \"negative\"},\n",
    "    {\"text\": train_examples[1]['text'], \"label\": \"positive\"},\n",
    "    {\"text\": train_examples[2]['text'], \"label\": \"negative\"},\n",
    "]\n",
    "\n",
    "prompt_parts = [\"Classify the sentiment of movie reviews as either 'positive' or 'negative'.\\n\"]\n",
    "for ex in mislabeled_examples:\n",
    "    prompt_parts.append(f\"\\nReview: {ex['text']}\\nSentiment: {ex['label']}\")\n",
    "\n",
    "prompt_parts.append(f\"\\n\\nReview: {test_examples[0]['text']}\\nSentiment:\")\n",
    "print(\"\".join(prompt_parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your predictions here\n",
    "mislabeled_predictions = [\n",
    "    \"\",  # Test 1\n",
    "    \"\",  # Test 2\n",
    "    \"\",  # Test 3\n",
    "    \"\",  # Test 4\n",
    "    \"\"   # Test 5\n",
    "]\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = sum([1 for pred, truth in zip(mislabeled_predictions, ground_truth) if pred == truth])\n",
    "accuracy = correct / len(ground_truth)\n",
    "\n",
    "print(f\"Mislabeled Examples Results:\")\n",
    "for i, (pred, truth) in enumerate(zip(mislabeled_predictions, ground_truth), 1):\n",
    "    status = \"✓\" if pred == truth else \"✗\"\n",
    "    print(f\"Test {i}: {status} Predicted={pred}, Expected={truth}\")\n",
    "print(f\"\\nAccuracy: {correct}/{len(ground_truth)} = {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 7: Chain-of-Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain-of-thought prompt with reasoning\n",
    "prompt = f\"\"\"Classify the sentiment of movie reviews as either 'positive' or 'negative'.\n",
    "Think step by step about the language used before making your classification.\n",
    "\n",
    "Review: \"{train_examples[0]['text']}\"\n",
    "Reasoning: The review uses positive descriptive words like \"talent\" and \"fame\" and describes energetic, entertaining actions.\n",
    "Sentiment: positive\n",
    "\n",
    "Review: \"{train_examples[1]['text']}\"\n",
    "Reasoning: The review uses negative words like \"clunker\" and sarcastic phrases like \"despite all evidence\" suggesting disappointment.\n",
    "Sentiment: negative\n",
    "\n",
    "Review: \"{test_examples[0]['text']}\"\n",
    "Reasoning:\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your predictions here (extract the sentiment from the full response)\n",
    "cot_predictions = [\n",
    "    \"\",  # Test 1\n",
    "    \"\",  # Test 2\n",
    "    \"\",  # Test 3\n",
    "    \"\",  # Test 4\n",
    "    \"\"   # Test 5\n",
    "]\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = sum([1 for pred, truth in zip(cot_predictions, ground_truth) if pred == truth])\n",
    "accuracy = correct / len(ground_truth)\n",
    "\n",
    "print(f\"Chain-of-Thought Results:\")\n",
    "for i, (pred, truth) in enumerate(zip(cot_predictions, ground_truth), 1):\n",
    "    status = \"✓\" if pred == truth else \"✗\"\n",
    "    print(f\"Test {i}: {status} Predicted={pred}, Expected={truth}\")\n",
    "print(f\"\\nAccuracy: {correct}/{len(ground_truth)} = {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Compare All Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# After filling in all predictions above, calculate accuracies\n",
    "def calc_accuracy(predictions):\n",
    "    if not any(predictions):  # Check if empty\n",
    "        return 0.0\n",
    "    correct = sum([1 for pred, truth in zip(predictions, ground_truth) if pred == truth])\n",
    "    return correct / len(ground_truth)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Experiment': [\n",
    "        'Zero-Shot',\n",
    "        'One-Shot',\n",
    "        'Three-Shot',\n",
    "        'Five-Shot',\n",
    "        'Shuffled',\n",
    "        'Mislabeled',\n",
    "        'Chain-of-Thought'\n",
    "    ],\n",
    "    'Accuracy': [\n",
    "        f\"{calc_accuracy(zero_shot_predictions):.1%}\",\n",
    "        f\"{calc_accuracy(one_shot_predictions):.1%}\",\n",
    "        f\"{calc_accuracy(three_shot_predictions):.1%}\",\n",
    "        f\"{calc_accuracy(five_shot_predictions):.1%}\",\n",
    "        f\"{calc_accuracy(shuffled_predictions):.1%}\",\n",
    "        f\"{calc_accuracy(mislabeled_predictions):.1%}\",\n",
    "        f\"{calc_accuracy(cot_predictions):.1%}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(results.to_string(index=False))\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}